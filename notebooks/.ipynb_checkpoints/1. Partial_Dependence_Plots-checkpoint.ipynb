{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-k7H1HOcN5pl",
    "tags": []
   },
   "source": [
    "# Building Explainable Machine Learning Models\n",
    "## Exercise: Partial Dependence Plot(PDP) & Individual Conditional Expectation(ICE) Plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5YK66yDN5pm"
   },
   "source": [
    "\n",
    "\n",
    "Feature Importance gives us the features that affect the predicions, the partial dependence plots on the other hand, tell us how a feature affects the predictions. PDPs can analyze the interaction between the target and the input feature of interest, while marginalizing all the other input features [1]. In other words. what happens to the target when one of the feature changes but all other variables are held constant. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "* [1]PDPs were introduced in the context of gradient boosting machines (GBM) by Friedman (2000)- Friedman, Jerome H. [“Greedy Function Approximation: A Gradient Boosting Machine.”](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full) Annals of Statistics 29: 1189–1232.\n",
    "    \n",
    "* Partial Dependence is calculated after a model has been fitted. So, let’s train and fit a Random Forest Classifier model on the training data.\n",
    "    \n",
    "* Also make sure that the model has a good predictive performance or in other words you have trained a good model before inspecting the importance of its features. There is no point in investigating a bad model.\n",
    "    </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCe7n3pEN5pn"
   },
   "source": [
    "During this exercise, you are going to be working with the Partial Dependence Plots(PDPs) on a preprocessed version of the [Adult Income Dataset ](https://archive.ics.uci.edu/ml/datasets/adult) using scikit-learn. You'll perform the following steps:\n",
    "\n",
    "1. Import and preprocess the dataset\n",
    "2. Train a Random Forest classifier on the data.\n",
    "3. Compute and plot one and two dimensional PDPs.\n",
    "4. Explain the results\n",
    "\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18PI_OfnN5pn"
   },
   "source": [
    "## Case Study\n",
    "\n",
    "### Predicting if income exceeds $50,000 per year based on 1994 US Census Data \n",
    "\n",
    " The Adult Income dataset, also known as the \"Census Income\" dataset, contains a variety of attributes or features that describe individuals in the dataset. The target variable is True if a person earns more than `$50K` annually and False if the earned income is ≤ $50K. The attributes of our dataset (preprocessed version) include:\n",
    "\n",
    "* **Age** : the age of the individual\n",
    "* **Workclass** : the type of employment the individual has (e.g. private, government, self-employed)\n",
    "* **Education** : the level of education the individual has achieved (e.g. high school, college, graduate degree)\n",
    "* **Marital-status** : the individual's marital status (e.g. married, divorced, single)\n",
    "* **Occupation** : the type of work the individual is employed in (e.g. sales, management, craft-repair)\n",
    "* **Race** : the individual's race (e.g. White, Black, Asian)\n",
    "* **Sex** : the individual's gender (e.g. Male, Female)\n",
    "* **Hours-per-week** : the number of hours the individual works per week\n",
    "* **Income** : the individual's income level (either <=50K or >50K)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "\n",
    "Note that the attributes might be different based on the source of the dataset and the version you are using. Also this dataset reflects bias and imbalance w.r.t gender and race. Be careful when using such datasets. In this notebook this dataset is only used for demo purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbiTGd_9N5po"
   },
   "source": [
    "## Importing Necessary Libraries\n",
    "\n",
    "Let’s start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffcHUn82N5po"
   },
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "import pandas as pd   \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "np.random.seed(123) #ensure reproducibility\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze5OEKjZN5po",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting up the styling for the plots in this notebook\n",
    "sns.set(style=\"white\", palette=\"colorblind\", font_scale=0.8, \n",
    "        rc={\"figure.figsize\":(10,8)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEwo7zv7N5pp"
   },
   "source": [
    "##  Reading in the Dataset\n",
    "\n",
    "Let’s read in the data and look at the first few rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dGOwCU0cN5pp",
    "outputId": "b824138f-807b-496a-9f33-3e7a3cbc0cb7"
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv('../data/Adult Income Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7Nw0JtkN5pq",
    "outputId": "c1f5f39c-a7f6-4e07-802f-3556176eabd3"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNiOO0FJN5pq"
   },
   "source": [
    "There are no missing values and no string columns in the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mrPkRVvNN5pr",
    "outputId": "91a36f77-d06e-45b3-8390-37518eb0c734"
   },
   "outputs": [],
   "source": [
    "# Inspecting Distribution of Target Variable\n",
    "\n",
    "data['income'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAi0lgkXN5pr"
   },
   "source": [
    "\n",
    "\n",
    "## Data Preparation \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX6JoBNzN5pr"
   },
   "source": [
    "### Train/Test Split\n",
    "Splitting the dataset into traininga and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKVnVtNJN5pr"
   },
   "outputs": [],
   "source": [
    "# Creating the target and the features column and splitting the dataset into test and train set.\n",
    "  \n",
    "X = data.iloc[:, :-1]  \n",
    "y = data.iloc[:, -1]  \n",
    "  \n",
    "# splitting the dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the categorical features\n",
    "\n",
    "We'll leave the numerical features as-is and only encode the categorical features using a OrdinalEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"age\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "categorical_features = X_train.columns.drop(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OrdinalEncoder(), categorical_features),\n",
    "        (\"num\", \"passthrough\", numerical_features),\n",
    "    ],\n",
    "    sparse_threshold=1,\n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform=\"pandas\")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBTFkkRrN5ps"
   },
   "source": [
    "## Training the classifier\n",
    "\n",
    "Now you will fit a [Random Forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and compute the [AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html) achieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOAkhm0TN5ps",
    "outputId": "c1bb0a48-fdd1-4019-de05-ab3f4dd73516"
   },
   "outputs": [],
   "source": [
    "# Training and fitting a Random Forest Model\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(n_estimators=100,max_depth=5,random_state=0)\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Glyygqd4N5ps"
   },
   "source": [
    "We now have our model and our predictions. Let’s now explore the different ways by which we can understand the model and its predictions in a more meaningful way. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHk-ixbNN5ps"
   },
   "source": [
    "## Calculating Partial Dependence plots\n",
    "\n",
    "Partial Dependence plots are calculated as follows:\n",
    "\n",
    "* Train a model\n",
    "* Set all the values for the `feature of interest` to a particluar value without touching any other variables.\n",
    "* Make predictions on the modified dataset using the previously trained model\n",
    "* Average over all the predictions. \n",
    "* Repeat for all the different values of the `feature of interest`.\n",
    "\n",
    "\n",
    "\n",
    "The `sklearn.inspection` module provides a convenience function `plot_partial_dependence` to create one-way and two-way partial dependence plots. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSkDWrKAN5ps",
    "tags": []
   },
   "source": [
    "## Plotting 1D PDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "95dXqdGlN5ps",
    "outputId": "f15d947e-9410-4bcb-dd49-564813e21ba4"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "common_params = {\n",
    "    \"subsample\": 50,\n",
    "    \"grid_resolution\": 10,\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "features_info = {\n",
    "    # features of interest\n",
    "    \"features\": [\"age\", \"hours_per_week\", \"workclass\", \"education\"],\n",
    "    # type of partial dependence plot\n",
    "    \"kind\": \"average\",\n",
    "    # information regarding categorical features\n",
    "    \"categorical_features\":categorical_features,\n",
    "}\n",
    "\n",
    "_, ax = plt.subplots(ncols=2, nrows=2, figsize=(9, 8), constrained_layout=True)\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    model,\n",
    "    X_train,\n",
    "    **features_info,\n",
    "    ax=ax,\n",
    "    **common_params,\n",
    ")\n",
    "\n",
    "_ = display.figure_.suptitle(\n",
    "    \"Partial dependence plots\",\n",
    "    fontsize=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ74cLouN5ps"
   },
   "source": [
    "## ICE Plots\n",
    "\n",
    "Partial dependence plots show the average effect of the features of interest while [Individual Conditional Expectation](https://arxiv.org/abs/1309.6392) plots visualize the dependence of the prediction on a feature for each sample separately, with one line per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "koZqW9iSN5ps",
    "outputId": "0a65719d-d454-48d0-d116-2714c91e504f"
   },
   "outputs": [],
   "source": [
    "common_params = {\n",
    "    \"subsample\": 50,\n",
    "    \"grid_resolution\": 10,\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "features_info = {\n",
    "    # features of interest\n",
    "    \"features\": [\"age\", \"hours_per_week\"],\n",
    "    # type of partial dependence plot\n",
    "    \"kind\": \"both\",\n",
    "}\n",
    "\n",
    "_, ax = plt.subplots(ncols=2, nrows=1, figsize=(8, 4), constrained_layout=True)\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    model,\n",
    "    X_train,\n",
    "    **features_info,\n",
    "    ax=ax,\n",
    "    **common_params,\n",
    ")\n",
    "\n",
    "_ = display.figure_.suptitle(\n",
    "    \"Partial dependence plots\",\n",
    "    fontsize=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7y6zKWd5N5pt",
    "tags": []
   },
   "source": [
    "### References\n",
    "\n",
    "[1] https://scikit-learn.org/stable/modules/partial_dependence.html\n",
    "\n",
    "[2] T. Hastie, R. Tibshirani and J. Friedman, [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn), Second Edition, Section 10.13.2, Springer, 2009.\n",
    "\n",
    "[3] C. Molnar, [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book), Section 5.1, 2019.\n",
    "\n",
    "[4] A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin, [Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation](https://arxiv.org/abs/1309.6392), Journal of Computational and Graphical Statistics, 24(1): 44-65, Springer, 2015.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of 1. Partial Dependence Plots.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
