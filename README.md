# Explainable Machine Learning Models - Workshop

### Making sense of opaque and complex models using Python

This repo is home to the code that accompanies a course with the same name on [O'Reilly Learning Platform](https://learning.oreilly.com/live-events/explainable-machine-learning-models/0636920061943/0636920061942/)

## Workshop

AI models are making predictions that affect people’s lives, so ensuring that they’re fair and unbiased must be an industry imperative. One way to ensure fairness is to discover a model’s mispredictions and analyze and fix the underlying causes. Some machine learning methods, like logistic regression and decision trees, are interpretable, but they aren’t highly accurate in their predictions. Others, like boosted trees and deep neural nets, are more accurate, but the logic behind their predictions can’t be clearly identified or explained, making it more difficult to spot and fix bias.

Join expert Parul Prandey to get the lowdown on commonly used techniques like SHAP values, LIME, partial dependence plots, and more that can help you explain the inexplicable in these models and ensure responsible machine learning. You’ll gain an understanding of the intuition behind the techniques and learn how to implement them in Python. Using case studies, you’ll discover how to extract the most important features and values of a model’s predictions to discover why a particular person has been denied a bank loan or is more susceptible to a heart attack. Finally, you’ll examine the vulnerabilities and shortcomings of these methods and discuss the road ahead.

## Hands-on learning with Jupyter notebooks

All exercises and labs are provided as Jupyter notebooks—interactive documents that combine live code, equations, visualizations, and narrative text. There's nothing to install or configure; just click a link and get started! And you can revisit them anytime after class ends to practice and refine your skills.

## What you’ll learn and how you can apply it
**By the end of this live online course, you’ll understand:**

* The fundamental concept behind interpretability and explainability
* An overview of various explanability techniques
* How to interpret opaque machine learning models using post hoc techniques in Python
* The challenges and strengths associated with each technique

**And you’ll be able to:**

* Intuit explanations for the predictions of machine learning models
* Implement techniques like permutation importance, partial dependence plots, SHAP, and LIME
* Interpret and visualize the output of any machine learning model

---

## This live event is for you because…

-   You’re a data science professional who wants to understand how a machine learning model makes predictions.
-   You’re concerned about the ethical and moral implications of machine learning.

### Prerequisites

-   No preparation or local installation needed—all exercises will be provided using  [Jupyter notebooks](https://learning.oreilly.com/interactive/?classification=jupyter-notebook)
-   Familiarity with Python and machine learning libraries

**Recommended preparation:**

-   Read  [_Responsible Machine Learning_](https://learning.oreilly.com/library/view/responsible-machine-learning/9781492090878/)  (book)

**Recommended follow-up:**

-   Read  [_An Introduction to Machine Learning Interpretability_](https://learning.oreilly.com/library/view/an-introduction-to/9781098115487), second edition (book)
-   Watch  [_Opening the Black Box: Explainable AI (XAI)_](https://learning.oreilly.com/videos/strata-data-conference/9781492050568/9781492050568-video325089/)  (conference session)

---

## Schedule

The timeframes are only estimates and may vary according to how the class is progressing.

**Getting started (10 minutes)**

-   Presentation: The motivation for explaining opaque and complex models
-   Group discussion: Common examples of bias in machine learning
-   Q&A

**Understanding the basics (25 minutes)**

-   Presentation: Interpretability versus explainability; differentiating between opaque and clear models; local versus global explanations
-   Q&A

**Permutation importance (30 minutes)**

-   Presentation: Permutation importance fundamentals; intuiting permutation importance Jupyter notebook: Explain Model Predictions and Visualize the Results Using the Permutation Importance Technique
-   Q&A
-   Break

**Partial dependence plots (30 minutes)**

-   Presentation: Partial dependence plot concepts and variants
-   Jupyter notebook: Explain and Visualize Model Predictions Using Partial Dependence Plots
-   Q&A

**SHAP: Shapley additive explanations and summary plots (30 minutes)**

-   Presentation: The importance of SHAP values
-   Jupyter notebook: Implement the SHAP Technique in Python
-   Q&A
-   Break

**LIME: Local interpretable model-agnostic explanations (30 minutes)**

-   Presentation: LIME fundamentals; intuiting LIME values
-   Jupyter notebook: Use and Implement LIME for Tabular, Text, and Image Data in Python
-   Q&A

**Counterfactual explanations (20 minutes)**

-   Presentation: Counterfactuals fundamentals; intuiting counterfactual explanations
-   Jupyter notebook: Assess the Importance of a Model Prediction Using Counterfactual Explanations in Python

**Wrap-up and Q&A (5 minutes)**

